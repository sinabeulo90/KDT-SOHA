{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd072f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581",
   "display_name": "Python 3.9.2 64-bit ('py3': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "72f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dqn_v1.dqn import *\n",
    "from simulator.simulator import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_count = 8\n",
    "action_size = 6\n",
    "\n",
    "skip_frame = 5\n",
    "stack_frame = 10\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epsilon_init = 0.3\n",
    "memory_maxlen = int(1e+4)\n",
    "\n",
    "epsilon_decay = 1e-5\n",
    "discount_factor = 0.9\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "update_target_episode = 50\n",
    "\n",
    "max_step = 3000\n",
    "\n",
    "model_save_episode = 50\n",
    "model_save_comment = None\n",
    "\n",
    "model_load_episode = None\n",
    "model_load_comment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(sensor_count, stack_frame, action_size)\n",
    "agent = DQNAgent(model, learning_rate, epsilon_init, skip_frame, stack_frame, memory_maxlen)\n",
    "\n",
    "agent.epsilon_min = 1.0\n",
    "\n",
    "if model_load_comment:\n",
    "    agent.model_load(model_load_episode, model_load_comment, eval=False)\n",
    "\n",
    "env = Simulator(map=\"rally_map.png\", fps=5, reward_radius=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "episode = 0\n",
    "\n",
    "try:\n",
    "    while True: #episode < max_episode:\n",
    "        step = 0\n",
    "        gear_drive_cnt = 0\n",
    "        gear_reverse_cnt = 0\n",
    "\n",
    "        losses = []\n",
    "        max_qs = []\n",
    "        rewards = []\n",
    "        \n",
    "        obs, reward, _ = env.reset()\n",
    "        gear = env.BREAK\n",
    "\n",
    "        agent.reset(obs)\n",
    "\n",
    "        state = agent.skip_stack_frame(obs)\n",
    "\n",
    "        while not env.is_done and step < max_step:\n",
    "            # env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # 조향각 조정\n",
    "            if action % 3 == 0:\n",
    "                steering_deg = -env.max_steering_deg\n",
    "            elif action % 3 == 1:\n",
    "                steering_deg = 0\n",
    "            elif action % 3 == 2:\n",
    "                steering_deg = env.max_steering_deg\n",
    "            \n",
    "            # 기어 조정\n",
    "            if action // 3 == 0:\n",
    "                if gear != env.DRIVE:\n",
    "                    env.car.reset()\n",
    "                gear = env.DRIVE\n",
    "                gear_drive_cnt += 1\n",
    "            elif action // 3 == 1:\n",
    "                if gear != env.REVERSE:\n",
    "                    env.car.reset()\n",
    "                gear = env.REVERSE\n",
    "                gear_reverse_cnt += 1\n",
    "                \n",
    "            next_obs, reward, done = env.step(gear, steering_deg)\n",
    "\n",
    "            next_state = agent.skip_stack_frame(next_obs)\n",
    "                \n",
    "            done = 1 if done else 0\n",
    "\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "\n",
    "            if len(agent.experience_memory) >= batch_size * 5:\n",
    "                for _ in range(10):\n",
    "                    loss, max_q = agent.train_model(discount_factor, batch_size)\n",
    "                    losses.append(loss)\n",
    "                    max_qs.append(max_q)\n",
    "\n",
    "            if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon -= epsilon_decay\n",
    "\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "        print(\"episode: {: >10d} | loss: {: >10.2f} | rewards: {: >7} | epsilon: {: >7.2f} | drive/reverse: ({: >7d} / {: >7d})\".format(\n",
    "            episode, np.mean(losses), np.sum(rewards), agent.epsilon, gear_drive_cnt, gear_reverse_cnt))\n",
    "\n",
    "        # target 네트워크 업데이트\n",
    "        if episode % update_target_episode == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        # 모델 저장\n",
    "        if episode % model_save_episode == 0:\n",
    "            agent.model_save(episode, model_save_comment)\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        writer.add_scalar(\"DQN/loss\", np.mean(losses), episode)\n",
    "        writer.add_scalar(\"DQN/max_q\", np.mean(max_qs), episode)\n",
    "        writer.add_scalar(\"DQN/reward\", np.sum(rewards), episode)\n",
    "        writer.add_scalar(\"DQN/epsilon\", agent.epsilon, episode)\n",
    "        writer.flush()\n",
    "\n",
    "finally:\n",
    "    writer.close()"
   ]
  }
 ]
}