{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd072f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581",
   "display_name": "Python 3.9.2 64-bit ('py3': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "72f82c1a1813237a015b4f4b2159de22a2fabef90219bd259332cc8881f19581"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ver2.dqn import DQNAgent\n",
    "from ver2.model import LinearModel\n",
    "from ver2.replay_buffer import ReplayBuffer\n",
    "from simulator.simulator import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_count = 8\n",
    "action_size = 6\n",
    "\n",
    "skip_frame = 5\n",
    "stack_frame = 10\n",
    "\n",
    "discount_factor = 0.99\n",
    "learning_rate = 1e-4\n",
    "\n",
    "epsilon = 1.0\n",
    "decay = 1e-4\n",
    "\n",
    "memory_start = 1000\n",
    "memory_maxlen = int(1e+4)\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "update_target_episode = 1000\n",
    "\n",
    "max_step = 3000\n",
    "\n",
    "model_save_episode = 50\n",
    "model_save_comment = None\n",
    "\n",
    "model_load_episode = None\n",
    "model_load_comment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(sensor_count, stack_frame, action_size)\n",
    "target_model = LinearModel(sensor_count, stack_frame, action_size)\n",
    "\n",
    "agent = DQNAgent(model, target_model, discount_factor, learning_rate, epsilon)\n",
    "\n",
    "if model_load_comment:\n",
    "    agent.load_model(model_load_episode, model_load_comment, eval=False)\n",
    "\n",
    "env = Simulator(map=\"rally_map.png\", fps=5, reward_radius=100)\n",
    "replay_buffer = ReplayBuffer(stack_frame, memory_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "episode = 1\n",
    "\n",
    "is_decay = False\n",
    "\n",
    "try:\n",
    "    while True: #episode < max_episode:\n",
    "        step = 0\n",
    "        gear_drive_cnt = 0\n",
    "        gear_reverse_cnt = 0\n",
    "\n",
    "        losses = []\n",
    "        max_qs = []\n",
    "        rewards = []\n",
    "        \n",
    "        obs, reward, _ = env.reset()\n",
    "        gear = env.BREAK\n",
    "\n",
    "        replay_buffer.reset()\n",
    "        state = replay_buffer.get_state(obs)\n",
    "        \n",
    "        while not env.is_done and step < max_step:\n",
    "            # env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # 조향각 조정\n",
    "            if action % 3 == 0:\n",
    "                steering_deg = -env.max_steering_deg\n",
    "            elif action % 3 == 1:\n",
    "                steering_deg = 0\n",
    "            elif action % 3 == 2:\n",
    "                steering_deg = env.max_steering_deg\n",
    "            \n",
    "            # 기어 조정\n",
    "            if action // 3 == 0:\n",
    "                if gear != env.DRIVE:\n",
    "                    env.car.reset()\n",
    "                gear = env.DRIVE\n",
    "                gear_drive_cnt += 1\n",
    "            elif action // 3 == 1:\n",
    "                if gear != env.REVERSE:\n",
    "                    env.car.reset()\n",
    "                gear = env.REVERSE\n",
    "                gear_reverse_cnt += 1\n",
    "                \n",
    "            reward = 0\n",
    "            next_state = None\n",
    "            done = False\n",
    "\n",
    "            # Skipping\n",
    "            for _ in range(skip_frame):\n",
    "                _obs, _reward, _done = env.step(gear, steering_deg)\n",
    "                reward += _reward\n",
    "                if _done:\n",
    "                    done = _done\n",
    "                    next_state = replay_buffer.get_state(_obs)\n",
    "                    break\n",
    "            else:\n",
    "                next_state = replay_buffer.get_state(_obs)\n",
    "                \n",
    "            done = 0.0 if done else 1.0\n",
    "\n",
    "            replay_buffer.put((state, action, reward, next_state, done))\n",
    "\n",
    "            if replay_buffer.size() >= memory_start:\n",
    "                is_decay = True\n",
    "                for _ in range(10):\n",
    "                    loss, max_q = agent.train_model(replay_buffer, batch_size)\n",
    "                    losses.append(loss)\n",
    "                    max_qs.append(max_q)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "        print(\"episode: {: >7d} | loss: {: >7.2f} | rewards: {: >7} | epsilon: {: >7.2f}% | drive/reverse: ({: >7d} / {: >7d})\".format(\n",
    "            episode, np.mean(losses), np.sum(rewards), agent._epsilon * 100, gear_drive_cnt, gear_reverse_cnt))\n",
    "\n",
    "        # target 네트워크 업데이트\n",
    "        if episode % update_target_episode == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        # 모델 저장\n",
    "        if episode % model_save_episode == 0:\n",
    "            agent.save_model(episode, model_save_comment)\n",
    "\n",
    "        # Epsilon decay\n",
    "        if is_decay:\n",
    "            agent.epsilon_decay(decay)\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        writer.add_scalar(\"DQN/loss\", np.mean(losses), episode)\n",
    "        writer.add_scalar(\"DQN/max_q\", np.mean(max_qs), episode)\n",
    "        writer.add_scalar(\"DQN/reward\", np.sum(rewards), episode)\n",
    "        writer.add_scalar(\"DQN/epsilon\", agent._epsilon, episode)\n",
    "        writer.flush()\n",
    "\n",
    "finally:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}